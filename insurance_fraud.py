# -*- coding: utf-8 -*-
"""insurance_fraud.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sOaYxaly_srYLJhyen-Cw30AJ3htaLxS
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import seaborn as sns
from sklearn.metrics import confusion_matrix

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
from sklearn.metrics import log_loss

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

dataset=pd.read_csv("insurance_claims.csv")

dataset

dataset.columns

dataset=dataset.replace("?",np.nan)

dataset

missing_col=[]
for col in dataset:
  if (len(dataset[col].isnull().value_counts())==2):
    print(col)
    missing_col.append(col)

for x in missing_col:
  print(dataset[x].unique())

for x in missing_col:
  dataset[x]=dataset[x].replace(np.NaN,dataset[x].mode()[0])

dataset

for x in missing_col:
  print(dataset[x].unique())

for x in dataset.columns:
  print(x,":",len(dataset[x].unique()),":",dataset[x].dtypes)

def getdummies(data,limit):
  
  for x in data.columns:
    if (len(data[x].unique())<limit and data[x].dtypes=="object"):
      data[x]=pd.get_dummies(data[x])
  return data

dataset=getdummies(dataset,7)

dataset

dataset.dtypes

a=[]
for x in dataset.columns:
  if (dataset[x].dtypes=="object"):
    a.append(x)

a

dataset=dataset.drop(a,axis=1)

dataset=dataset.drop(labels=["months_as_customer",'age','policy_number','auto_year','insured_sex','insured_relationship','insured_zip','policy_state',],axis=1)

def normalise(dataset,label):
  datanorm=((dataset-dataset.min())/(dataset.max()-dataset.min()))
  for x in label:
    datanorm[x]=dataset[x]
  return datanorm

dataset=normalise(dataset,['policy_csl','incident_type','collision_type','incident_severity','authorities_contacted','number_of_vehicles_involved','property_damage','bodily_injuries','witnesses','police_report_available','fraud_reported'])

dataset

dataset['fraud_reported'].value_counts()

sns.countplot(x='fraud_reported',data=dataset,palette='hls')
plt.show()

dataset.groupby('fraud_reported').mean()

dataset.groupby('fraud_reported').corr()

dataset.describe()

x=dataset.loc[:,dataset.columns !='fraud_reported']
y=dataset.loc[:,dataset.columns=='fraud_reported']
from imblearn.over_sampling import SMOTE
os= SMOTE(random_state=0)
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=0)
columns =x_train.columns
os_data_x,os_data_y=os.fit_sample(x_train,y_train)
os_data_x=pd.DataFrame(data=os_data_x,columns=columns)
os_data_y=pd.DataFrame(data=os_data_y,columns=['fraud_reported'])
print("length of oversampled data is:", len(os_data_x))
print("number of fraud reported yes in oversampled data:",len(os_data_y[os_data_y['fraud_reported']==0]))
print("number of fraud reported yes in oversampled data:",len(os_data_y[os_data_y['fraud_reported']==0]))

print("proportion of fraud detected in oversmapled data is: ",len(os_data_y[os_data_y['fraud_reported']==0])/len(os_data_x))

x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.3,random_state=0)
logreg=LogisticRegression()
logreg.fit(x_train,y_train)

y_pred=logreg.predict(x_test)
print('Accuracy of logistic regression classifier on test set: {: .2f}'.format(logreg.score(x_test,y_test)))

confusion=confusion_matrix(y_test,y_pred)

confusion

accuracy=accuracy_score(x_test,y_pred)

accuracy

print(classification_report(y_test,y_pred))

roc_auc_score(y_test,y_pred)

print(log_loss(y_test,y_pred))

